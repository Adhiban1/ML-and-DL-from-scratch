{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"images/nn.png\" alt=\"images/nn.png\" width=400px>\n",
    "<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dot product of Vector and Matrix**\n",
    "\n",
    "$$c_{k} = \\sum_{}^{i} a_{i}b_{ik}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_dot_mat(a, b):\n",
    "    c = []\n",
    "    for k in range(len(b[0])):\n",
    "        s = 0\n",
    "        for i in range(len(a)):\n",
    "            s += a[i] * b[i][k]\n",
    "        c.append(s)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "\n",
    "def rand_matrix(i, j):\n",
    "    return [[uniform(-1, 1) for _ in range(j)] for _ in range(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let us take,**\n",
    "\n",
    "- Input matrix $X$ of size (10, 10)\n",
    "- True weight1 matrix $W_{t1}$ of size (10, 5)\n",
    "- True bias1 $b_{t1}$\n",
    "- True weight2 matrix $W_{t2}$ of size (5, 1)\n",
    "- True bias1 $b_{t2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rand_matrix(10, 10)\n",
    "wt1 = rand_matrix(10, 5)\n",
    "bt1 = uniform(-1, 1)\n",
    "wt2 = rand_matrix(5, 1)\n",
    "bt2 = uniform(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to find out below weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = rand_matrix(10, 5)\n",
    "b1 = uniform(-1, 1)\n",
    "w2 = rand_matrix(5, 1)\n",
    "b2 = uniform(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\text{forward}_l =\\sum_{}^{k} \\left[ \\left[ \\sum_{}^{j} x_{j} . w1_{jk} + b1 \\right]_k . w2_{kl} \\right] + b2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, w1, w2, b1, b2):\n",
    "    z1 = [i+b1 for i in vec_dot_mat(x, w1)]\n",
    "    z2 = [i+b2 for i in vec_dot_mat(z1, w2)]\n",
    "    return z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [forward(i, wt1, wt2, bt1, bt2) for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modification of weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_w(w, index, h):\n",
    "    w[index[0]][index[1]] += h\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{loss} = \\frac{1}{N} \\sum_{}^{l} \\left\\{ y_l - \\sum_{}^{k} \\left[ \\left[ \\sum_{}^{j} x_{j} . w1_{jk} + b1 \\right]_k . w2_{kl} \\right] + b2 \\right\\}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, y, w1, w2, b1, b2):\n",
    "    l = 0\n",
    "    yp = forward(x, w1, w2, b1, b2)\n",
    "    for i in range(len(y)):\n",
    "        l += (y[i] - yp[i]) ** 2\n",
    "    l /= len(y)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{overall\\_loss} = \\frac{1}{N^2} \\sum_{}^{i} \\sum_{}^{l} \\left\\{ y_{il} - \\sum_{}^{k} \\left[ \\left[ \\sum_{}^{j} x_{ij} . w1_{jk} + b1 \\right]_k . w2_{kl} \\right] + b2 \\right\\}^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_loss(x, y, w1, w2, b1, b2):\n",
    "    yp = [forward(i, w1, w2, b1, b2) for i in x]\n",
    "    l = 0\n",
    "    for i in range(len(y)):\n",
    "        for j in range(len(y[0])):\n",
    "            l += (y[i][j] - yp[i][j]) ** 2\n",
    "    l /= len(y) * len(y[0])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{grad}_w = \\frac{\\text{loss}(mod_w(w, (i, j), h)) - \\text{loss}(mod_w(w, (i, j), -h))}{2h}$$\n",
    "\n",
    "$$\\text{grad}_b = \\frac{\\text{loss}(b+h) - \\text{loss}(b-h)}{2h}$$\n",
    "\n",
    "$$w1 \\leftarrow w1 - lr . \\triangle w1$$\n",
    "\n",
    "$$w2 \\leftarrow w2 - lr . \\triangle w2$$\n",
    "\n",
    "$$b1 \\leftarrow b1 - lr . \\triangle b1$$\n",
    "\n",
    "$$b2 \\leftarrow b2 - lr . \\triangle b2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x, y, w1, w2, b1, b2, h, lr):\n",
    "    dw1 = []\n",
    "    for i in range(len(w1)):\n",
    "        temp = []\n",
    "        for j in range(len(w1[0])):\n",
    "            temp.append((loss(x, y, mod_w(w1, (i, j), h), w2, b1, b2) \\\n",
    "                - loss(x, y, mod_w(w1, (i, j), -h), w2, b1, b2)) / (2 * h))\n",
    "        dw1.append(temp)\n",
    "    \n",
    "    dw2 = []\n",
    "    for i in range(len(w2)):\n",
    "        temp = []\n",
    "        for j in range(len(w2[0])):\n",
    "            temp.append((loss(x, y, w1, mod_w(w2, (i, j), h), b1, b2) \\\n",
    "                - loss(x, y, w1, \n",
    "                       mod_w(w2, (i, j), -h), b1, b2)) / (2 * h))\n",
    "        dw2.append(temp)\n",
    "    \n",
    "    db1 = (loss(x, y, w1, w2, b1+h, b2) \\\n",
    "           - loss(x, y, w1, w2, b1-h, b2)) / (2 * h)\n",
    "    db2 = (loss(x, y, w1, w2, b1, b2+h) \\\n",
    "           - loss(x, y, w1, w2, b1, b2-h)) / (2 * h)\n",
    "\n",
    "    for i in range(len(w1)):\n",
    "        for j in range(len(w1[0])):\n",
    "            w1[i][j] -= lr * dw1[i][j]\n",
    "\n",
    "    for i in range(len(w2)):\n",
    "        for j in range(len(w2[0])):\n",
    "            w2[i][j] -= lr * dw2[i][j]\n",
    "    \n",
    "    b1 -= lr * db1\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    return w1, w2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inital loss: 13.28357350279802\n",
      "Lowest loss: 7.2511179192521535e-06\n",
      "Final loss: 7.2511179192521535e-06\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "lr = 0.01\n",
    "h = 0.001\n",
    "opt_w_b = (w1, w2, b1, b2)\n",
    "lowest_loss = overall_loss(x, y, w1, w2, b1, b2)\n",
    "print('Inital loss: {}'.format(lowest_loss))\n",
    "for _ in range(epochs):\n",
    "    for x_row, y_row in zip(x,y):\n",
    "        w1, w2, b1, b2 = grad(x_row, y_row, w1, w2, b1, b2, h, lr)\n",
    "    l = overall_loss(x, y, w1, w2, b1, b2)\n",
    "    if l < lowest_loss:\n",
    "        lowest_loss = l\n",
    "        opt_w_b = (w1, w2, b1, b2)\n",
    "print('Lowest loss: {}'.format(lowest_loss))\n",
    "print('Final loss: {}'.format(overall_loss(x, y, w1, w2, b1, b2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rust code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```rust\n",
    "use rand::Rng;\n",
    "\n",
    "fn main() {\n",
    "    let mut rng = rand::thread_rng();\n",
    "\n",
    "    fn vec_dot_mat(a:&Vec<f64>, b:&Vec<Vec<f64>>) -> Vec<f64> {\n",
    "        let mut c = vec![];\n",
    "        for k in 0..b[0].len() {\n",
    "            let mut s = 0.0;\n",
    "            for j in 0..a.len() {\n",
    "                s += a[j] * b[j][k];\n",
    "            }\n",
    "            c.push(s);\n",
    "        }\n",
    "        c\n",
    "    }\n",
    "    \n",
    "    fn rand_matrix(i:usize, j:usize) -> Vec<Vec<f64>> {\n",
    "        let mut rng = rand::thread_rng();\n",
    "        let mut m = vec![vec![0.0;j];i];\n",
    "        for a in 0..i {\n",
    "            for b in 0..j {\n",
    "                m[a][b] = rng.gen_range(-1.0..=1.0);\n",
    "            }\n",
    "        }\n",
    "        m\n",
    "    }\n",
    "\n",
    "    let x = rand_matrix(10, 10);\n",
    "    let wt1 = rand_matrix(10, 5);\n",
    "    let bt1 = rng.gen_range(-1.0..=1.0);\n",
    "    let wt2 = rand_matrix(5, 1);\n",
    "    let bt2 = rng.gen_range(-1.0..=1.0);\n",
    "    \n",
    "    let mut w1 = rand_matrix(10, 5);\n",
    "    let mut b1 = rng.gen_range(-1.0..=1.0);\n",
    "    let mut w2 = rand_matrix(5, 1);\n",
    "    let mut b2 = rng.gen_range(-1.0..=1.0);\n",
    "    \n",
    "    fn forward(x:&Vec<f64>, w1:&Vec<Vec<f64>>, w2:&Vec<Vec<f64>>, b1:&f64, b2:&f64) -> Vec<f64> {\n",
    "        let z1:Vec<f64> = vec_dot_mat(x, w1).iter().map(|i|i+b1).collect();\n",
    "        let z2 = vec_dot_mat(&z1, w2).iter().map(|i|i+b2).collect();\n",
    "        z2\n",
    "    }\n",
    "    \n",
    "    let y:Vec<Vec<f64>> = x.iter().map(|i| forward(i, &wt1, &wt2, &bt1, &bt2)).collect();\n",
    "\n",
    "    fn mod_w(w:&Vec<Vec<f64>>, index:(usize, usize), h:f64) -> Vec<Vec<f64>>{\n",
    "        let mut w1 = w.clone();\n",
    "        w1[index.0][index.1] += h;\n",
    "        w1\n",
    "    }\n",
    "\n",
    "    fn loss(x:&Vec<f64>, y:&Vec<f64>, w1:&Vec<Vec<f64>>, w2:&Vec<Vec<f64>>, b1:&f64, b2:&f64) -> f64 {\n",
    "        let mut l = 0.0;\n",
    "        let yp = forward(x, w1, w2, b1, b2);\n",
    "        for i in 0..y.len() {\n",
    "            l += (y[i] - yp[i]).powi(2);\n",
    "        }\n",
    "        l /= y.len() as f64;\n",
    "        l\n",
    "    }\n",
    "    \n",
    "    fn overall_loss(x:&Vec<Vec<f64>>, y:&Vec<Vec<f64>>, w1:&Vec<Vec<f64>>, w2:&Vec<Vec<f64>>, b1:&f64, b2:&f64) -> f64 {\n",
    "        let mut yp = vec![];\n",
    "        for i in x {\n",
    "            let temp = forward(i, w1, w2, b1, b2);\n",
    "            yp.push(temp);\n",
    "        }\n",
    "        let mut l = 0.0;\n",
    "        for i in 0..y.len() {\n",
    "            for j in 0..y[0].len() {\n",
    "                l += (y[i][j] - yp[i][j]).powi(2);\n",
    "            }\n",
    "        }\n",
    "        l /= (y.len() * y[0].len()) as f64;\n",
    "        l\n",
    "    }\n",
    "\n",
    "    fn grad(x:&Vec<f64>, y:&Vec<f64>, w1:&Vec<Vec<f64>>, w2:&Vec<Vec<f64>>, b1:&f64, b2:&f64, h:f64) -> (Vec<Vec<f64>>, Vec<Vec<f64>>, f64, f64) {\n",
    "        let mut dw1 = vec![];\n",
    "        for i in 0..w1.len() {\n",
    "            let mut temp = vec![];\n",
    "            for j in 0..w1[0].len() {\n",
    "                temp.push((loss(x, y, &mod_w(w1, (i, j), h), w2, b1, b2) - loss(x, y, &mod_w(w1, (i, j), -h), w2, b1, b2)) / (2.0 * h));\n",
    "            }\n",
    "            dw1.push(temp);\n",
    "        }\n",
    "\n",
    "        let mut dw2 = vec![];\n",
    "        for i in 0..w2.len() {\n",
    "            let mut temp = vec![];\n",
    "            for j in 0..w2[0].len() {\n",
    "                temp.push((loss(x, y, w1, &mod_w(w2, (i, j), h), b1, b2) - loss(x, y, w1, &mod_w(w2, (i, j), -h), b1, b2)) / (2.0 * h));\n",
    "            }\n",
    "            dw2.push(temp);\n",
    "        }\n",
    "\n",
    "        let db1 = (loss(x, y, w1, w2, &(b1+h), b2) - loss(x, y, w1, w2, &(b1-h), b2)) / (2.0 * h);\n",
    "        let db2 = (loss(x, y, w1, w2, b1, &(b2+h)) - loss(x, y, w1, w2, b1, &(b2-h))) / (2.0 * h);\n",
    "\n",
    "        (dw1, dw2, db1, db2)\n",
    "    }\n",
    "\n",
    "    let epochs = 1000;\n",
    "    let lr = 0.01;\n",
    "    let h = 0.001;\n",
    "    let mut opt_w_b = (w1.clone(), w2.clone(), b1.clone(), b2.clone());\n",
    "    let mut lowest_loss = overall_loss(&x, &y, &w1, &w2, &b1, &b2);\n",
    "    println!(\"Inital loss: {}\", lowest_loss);\n",
    "    for _ in 0..epochs {\n",
    "        for index in 0..x.len() {\n",
    "            let (dw1, dw2, db1, db2) = grad(&x[index], &y[index], &w1, &w2, &b1, &b2, h);\n",
    "            for i in 0..w1.len() {\n",
    "                for j in 0..w1[0].len() {\n",
    "                    w1[i][j] -= lr * dw1[i][j];\n",
    "                }\n",
    "            }\n",
    "    \n",
    "            for i in 0..w2.len() {\n",
    "                for j in 0..w2[0].len() {\n",
    "                    w2[i][j] -= lr * dw2[i][j];\n",
    "                }\n",
    "            }\n",
    "    \n",
    "            b1 -= lr * db1;\n",
    "            b2 -= lr * db2;\n",
    "        }\n",
    "        let l = overall_loss(&x, &y, &w1, &w2, &b1, &b2);\n",
    "        if l < lowest_loss {\n",
    "            lowest_loss = l;\n",
    "            opt_w_b = (w1.clone(), w2.clone(), b1.clone(), b2.clone());\n",
    "        }\n",
    "    }\n",
    "    println!(\"Lowest loss: {}\", lowest_loss);\n",
    "    println!(\"Final loss: {}\", overall_loss(&x, &y, &w1, &w2, &b1, &b2));\n",
    "    // println!(\"{:?}\", opt_w_b);\n",
    "}\n",
    "```\n",
    "\n",
    "**output:**\n",
    "\n",
    "```\n",
    "Inital loss: 14.449090232922213\n",
    "Lowest loss: 0.000005498838681749966\n",
    "Final loss: 0.000005498838681749966\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
