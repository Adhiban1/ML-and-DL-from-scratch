{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linearized Weight Representation in Neural Networks\n",
    "\n",
    "In this documentation, we'll explore a unique approach to representing weights in neural networks, followed by a detailed explanation of the equations involved in the backpropagation algorithm using this representation.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Linearized Weight Representation\n",
    "\n",
    "Traditionally, the weights in neural networks are represented as matrices for each layer. However, in this approach, we linearize the weights into a flat array. The linearization simplifies mathematical formulations and aligns with the conventions of optimization algorithms used in deep learning frameworks.\n",
    "\n",
    "I attempted a different approach by using a flat weight array $w$ and a flat bias array $b$. The weights are accessed as matrices during computations, providing a balance between computational efficiency and simplicity in formulation.\n",
    "\n",
    "## Equations\n",
    "\n",
    "Let's delve into the equations that govern the backpropagation algorithm with this linearized weight representation.\n",
    "\n",
    "### Weight and Bias Lengths\n",
    "\n",
    "$$w\\_len = l_1.l_2 + l_2.l_3 + ... + l_{n-1}.l_n$$\n",
    "\n",
    "$$b\\_len = l_2 + l_3 + ... + l_n$$\n",
    "\n",
    "These equations define the lengths of the weight and bias arrays based on the layer sizes $l_1, l_2, ..., l_n$.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "The forward pass computes the output of each layer using the linearized weights:\n",
    "\n",
    "$$z_{l_2} = f_2 \\left[ \\sum_{l_1=0}^{L_1-1} (x_{l_1}.w_{l_1 + L_1l_2}) + b_{l_2} \\right]$$\n",
    "\n",
    "This equation demonstrates how the output of the second layer is computed using the sigmoid activation function $f_2$.\n",
    "\n",
    "$$z_{l_3} = f_3 \\left[ \\sum_{l_2=0}^{L_2-1} (z_{l_2}.w_{l_2 + L_1L_2 + L_2 l_3}) + b_{L_2 + l_3} \\right]$$\n",
    "\n",
    "Similarly, the output of the third layer and subsequent layers are computed.\n",
    "\n",
    "$$z_{l_4} = f_4 \\left[ \\sum_{l_3=0}^{L_3-1} (z_{l_3}.w_{l_3 + L_1L_2 + L_2 L_3 + L_3 l_4}) + b_{L_2 + L_3 + l_4} \\right]$$\n",
    "\n",
    "$$\\vdots$$\n",
    "\n",
    "$$z_{l_n} = f_n \\left[ \\sum_{l_{n-1}=0}^{L_{n-1}-1} (z_{l_{n-1}}.w_{l_{n-1} + L_1L_2 + L_2 L_3 + ... + L_{n-2} L_{n-1} + L_{n-1} l_n}) + b_{L_2 + L_3 + ...+ L_{n-1} +l_n} \\right]$$\n",
    "\n",
    "$$l_{n-1} + L_1L_2 + L_2 L_3 + ... + L_{n-2} L_{n-1} + L_{n-1} l_n = l_{n-1} + \\sum_{r=2}^{n-1} L_{r-1} L_{r} + L_{n-1}l_n$$\n",
    "\n",
    "$$L_2 + L_3 + ...+ L_{n-1} +l_n = \\sum_{r=2}^{n-1}L_r + l_n$$\n",
    "\n",
    "$$z_{l_n} = f_n \\left[ \\sum_{l_{n-1}=0}^{L_{n-1}-1} (z_{l_{n-1}}.w_{l_{n-1} + \\sum_{r=2}^{n-1} L_{r-1} L_{r} + L_{n-1}l_n}) + b_{\\sum_{r=2}^{n-1}L_r + l_n} \\right]$$\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function measures the error between predicted and actual outputs:\n",
    "\n",
    "$$loss = \\frac{1}{L_n} \\sum_{l_n=0}^{L_n-1} (y_{l_n} - z_{l_n})^2$$\n",
    "\n",
    "### Weight Update\n",
    "\n",
    "The weights and biases are updated during the training process:\n",
    "\n",
    "$$w_{l_1 + L_1l_2} \\leftarrow w_{l_1 + L_1l_2} - \\text{lr} \\cdot \\frac{d(loss)}{dw_{l_1}}$$\n",
    "\n",
    "The derivatives with respect to weights are calculated using the chain rule of calculus.\n",
    "\n",
    "### Gradient Calculation\n",
    "\n",
    "The gradients are calculated during the backward pass:\n",
    "\n",
    "$$\\frac{\\partial(loss)}{\\partial(z_{l_n})} = -2(y_{l_n} - z_{l_n})$$\n",
    "\n",
    "$$\\frac{\\partial(z_{l_n})}{\\partial(z_{l_{n-1}})} =f_n' \\left[ \\sum_{l_{n-1}=0}^{L_{n-1}-1} (z_{l_{n-1}}.w_{l_{n-1} + \\sum_{r=2}^{n-1} L_{r-1} L_{r} + L_{n-1}l_n}) + b_{\\sum_{r=2}^{n-1}L_r + l_n} \\right].w_{l_{n-1} + \\sum_{r=2}^{n-1} L_{r-1} L_{r} + L_{n-1}l_n}$$\n",
    "\n",
    "$$\\frac{\\partial z_{l_4}}{\\partial z_{l_3}} = f_4' \\left[ \\sum_{l_3=0}^{L_3-1} (z_{l_3}.w_{l_3 + L_1L_2 + L_2 L_3 + L_3 l_4}) + b_{L_2 + L_3 + l_4} \\right].w_{l_3 + L_1L_2 + L_2 L_3 + L_3 l_4}$$\n",
    "\n",
    "$$\\frac{\\partial z_{l_3}}{\\partial z_{l_2}} = f_3' \\left[ \\sum_{l_2=0}^{L_2-1} (z_{l_2}.w_{l_2 + L_1L_2 + L_2 l_3}) + b_{L_2 + l_3} \\right].w_{l_2 + L_1L_2 + L_2 l_3}$$\n",
    "\n",
    "$$\\frac{\\partial z_{l_2}}{\\partial w_{l_1}} = f_2' \\left[ \\sum_{l_1=0}^{L_1-1} (x_{l_1}.w_{l_1 + L_1l_2}) + b_{l_2} \\right].x_{l_1}$$\n",
    "\n",
    "$$\\frac{d(loss)}{dw_{l_1}} = \\frac{\\partial(loss)}{\\partial(z_{l_n})} . \\frac{\\partial(z_{l_n})}{\\partial(z_{l_{n-1}})} \\dots \\frac{\\partial z_{l_4}}{\\partial z_{l_3}} . \\frac{\\partial z_{l_3}}{\\partial z_{l_2}} . \\frac{\\partial z_{l_2}}{\\partial w_{l_1}}$$\n",
    "\n",
    "These equations show the gradients of the loss with respect to the network parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing this method in code\n",
    "\n",
    "## Overview\n",
    "\n",
    "This Python code demonstrates a simple neural network training using the backpropagation algorithm. The network has one hidden layer (`L1`) with 50 neurons and an output layer (`L2`) with 5 neurons. The code uses the sigmoid activation function for the hidden layer.\n",
    "\n",
    "## Code Explanation\n",
    "\n",
    "<!-- Previous sections... -->\n",
    "\n",
    "### Equations\n",
    "\n",
    "The core equations in the backpropagation algorithm are explained below:\n",
    "\n",
    "#### Forward Pass:\n",
    "\n",
    "The forward pass computes the output of the neural network given an input `x_matrix`:\n",
    "\n",
    "$$\n",
    "z_{l_2} = f_2 \\left[ \\sum_{l_1=0}^{L_1-1} (x_{l_1}.w_{l_1 + L_1l_2}) + b_{l_2} \\right]\n",
    "$$\n",
    "\n",
    "Here, $z_{l_2}$ represents the output of the hidden layer, which is computed using the sigmoid activation function $f_2$. The weighted sum of inputs and biases is passed through the activation function.\n",
    "\n",
    "#### Loss Calculation:\n",
    "\n",
    "The loss is computed as the mean squared error between the predicted output `zl2` and the actual output `y`:\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\frac{1}{L_2} \\sum_{l_2=0}^{L_2-1} (y_{l_2} - z_{l_2})^2\n",
    "$$\n",
    "\n",
    "This loss function measures the discrepancy between the predicted and actual outputs.\n",
    "\n",
    "#### Backward Pass (Gradient Calculation):\n",
    "\n",
    "The gradients with respect to the weights and biases are calculated using the chain rule of calculus. The derivative of the sigmoid activation function is denoted as $f'_2$.\n",
    "\n",
    "$$\\frac{\\partial(loss)}{\\partial(z_{l_2})} = -\\frac{2}{L_2} (y_{l_2} - z_{l_2})$$\n",
    "\n",
    "$$\\frac{\\partial z_{l_2}}{\\partial w_{l_1 + L_1l_2}} = f_2' \\left[ \\sum_{l_1=0}^{L_1-1} (x_{l_1}.w_{l_1 + L_1l_2}) + b_{l_2} \\right].x_{l_1}$$\n",
    "\n",
    "This equation represents the chain rule for calculating the gradient of the output with respect to the input of the hidden layer.\n",
    "\n",
    "#### Weight Update:\n",
    "\n",
    "The weights and biases are updated in the opposite direction of the gradient to minimize the loss. The learning rate is denoted as `lr`.\n",
    "\n",
    "$$\n",
    "w_{l_1 + L_1l_2} \\leftarrow w_{l_1 + L_1l_2} - \\text{lr} \\cdot \\frac{\\partial \\text{loss}}{\\partial w_{l_1 + L_1l_2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_{l_2} \\leftarrow b_{l_2} - \\text{lr} \\cdot \\frac{\\partial \\text{loss}}{\\partial b_{l_2}}\n",
    "$$\n",
    "\n",
    "These equations update the weights and biases to reduce the loss during the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Explanation\n",
    "\n",
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary libraries are imported: `numpy` for numerical operations and `matplotlib` for plotting.\n",
    "\n",
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "L1 = 50\n",
    "L2 = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random seed is set for reproducibility, and the network architecture is defined with 50 neurons in the hidden layer (`L1`) and 5 neurons in the output layer (`L2`).\n",
    "\n",
    "### Input and Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_matrix = np.random.randn(1000, L1)\n",
    "wt = np.random.randn(L1 * L2)\n",
    "bt = np.random.randn(L2)\n",
    "f2 = lambda x: 1 / (1 + np.exp(-x))\n",
    "df2 = lambda x: f2(x) * (1 - f2(x))\n",
    "y_matrix = f2(x_matrix @ wt.reshape(L1, L2) + bt)\n",
    "\n",
    "del(wt)\n",
    "del(bt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A random input matrix `x_matrix` of shape (1000, 50) is generated. Weights `wt` and biases `bt` are initialized for the hidden layer. The sigmoid activation function `f2` and its derivative `df2` are defined. The output `y_matrix` is calculated using the sigmoid activation function.\n",
    "\n",
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(L1 * L2)\n",
    "dw = np.zeros_like(w)\n",
    "b = np.random.randn(L2)\n",
    "db = np.zeros_like(b)\n",
    "zl2 = np.zeros(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight and bias matrices for training are initialized. `dw` and `db` are used to store gradients, and `zl2` holds the intermediate values for the hidden layer.\n",
    "\n",
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1639518888727417\n",
      "Loss: 0.0044892627238109955\n",
      "Loss: 4.7578049267731775e-06\n",
      "Loss: 5.567543772289793e-06\n",
      "Loss: 3.05669317549518e-06\n",
      "Loss: 2.221313599958404e-06\n",
      "Loss: 1.783741760239858e-06\n",
      "Loss: 1.4484989888879683e-06\n",
      "Loss: 1.1881932321334e-06\n",
      "Loss: 9.855031262001033e-07\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "lr = 0.1\n",
    "epochs = 10\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    for x, y in zip(x_matrix, y_matrix):\n",
    "        loss = 0\n",
    "        for l2 in range(L2):\n",
    "            s2 = b[l2]\n",
    "            for l1 in range(L1):\n",
    "                s2 += x[l1] * w[l1 + L1 * l2]\n",
    "            zl2[l2] = f2(s2)\n",
    "\n",
    "            loss += (y[l2] - zl2[l2]) ** 2\n",
    "\n",
    "            for l1 in range(L1):\n",
    "                dw[l1 + L1 * l2] = -2 * (y[l2] - zl2[l2]) * df2(s2) * x[l1]\n",
    "\n",
    "            db[l2] = -2 * (y[l2] - zl2[l2]) * df2(s2)\n",
    "        for l2 in range(L2):\n",
    "            for l1 in range(L1):\n",
    "                w[l1 + L1 * l2] = w[l1 + L1 * l2] - lr * dw[l1 + L1 * l2]\n",
    "            b[l2] = b[l2] - lr * db[l2]\n",
    "\n",
    "        loss /= L2\n",
    "    print(f'Loss: {loss}')\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code runs a training loop for a specified number of epochs. For each input-output pair, it calculates the loss, computes gradients using backpropagation, and updates the weights and biases accordingly. The loss values are printed for each epoch, and the loss curve is plotted at the end of training.\n",
    "\n",
    "### Plotting Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code generates a plot showing the decrease in loss over epochs, indicating the convergence of the network.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This code provides a simple implementation of a neural network training using the backpropagation algorithm. It serves as a starting point for understanding the mechanics of backpropagation and weight updates in a basic neural network architecture.\n",
    "\n",
    "This file provides a detailed explanation of the code, including the network architecture, input initialization, training parameters, and the training loop. It also explains the significance of the loss curve plot at the end of the training process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
